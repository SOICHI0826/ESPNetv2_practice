# -*- coding: utf-8 -*-
"""cnn_utils.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17bnundiAztE8B_agpY2ZgodM1Ka_zN42
"""

from google.colab import drive 
drive.mount('/content/drive')

import os
os.chdir("/content/drive/My Drive/ESPNetv2")

import sys
sys.path.append("/content/drive/My Drive/ESPNetv2")

import torch.nn as nn
import torch
import torch.nn.functional as F

#畳み込み層＋バッチ正規化＋活性化
class CBR(nn.Module):
  def __init__(self, nIn, nOut, k_size, stride = 1, groups = 1):
    super().__init__()
    padding = int((k_size - 1) / 2)
    self.conv = nn.Conv2d(nIn, nOut, k_size, stride = stride, padding = padding, bias = False, groups = groups)
    self.bn = nn.BatchNorm2d(nOut)
    self.act = nn.PReLU(nOut)

  def forward(self, input):
    output = self.conv(input)
    output = self.bn(output)
    output = self.act(output)

    return output

#バッチ正規化＋活性化
class BR(nn.Module):
  def __init__(self, nOut):
    super().__init__()
    self.bn = nn.BatchNorm2d(nOut)
    self.act = nn.PReLU(nOut)
  
  def forward(self, input):
    output = self.bn(input)
    output = self.act(output)

    return output

#畳み込みグループ化＋バッチ正規化
class CB(nn.Module):
  def __init__(self, nIn, nOut, k_size, stride = 1, groups = 1):
    super().__init__()
    padding = int((k_size - 1) / 2)
    self.conv = nn.Conv2d(nIn, nOut, k_size, stride = stride, padding = padding, bias = False, groups = groups)
    self.bn = nn.BatchNorm2d(nOut)
  
  def forward(self, input):
    output = self.conv(input)
    output = self.bn(output)

    return output

#畳み込み
class Conv(nn.Module):
  def __init__(self, nIn, nOut, k_size, stride = 1, groups = 1):
    super().__init__()
    padding = int((k_size - 1) / 2)
    self.conv = nn.Conv2d(nIn, nOut, k_size, stride = stride, padding = padding, bias = False, groups = groups)
  
  def forward(self, input):
    output = self.conv(input)

    return output

#拡張畳み込み
class CDilated(nn.Module):
  def __init__(self, nIn, nOut, k_size, stride = 1, d = 1, groups = 1):
    super().__init__()
    padding = int((k_size - 1) / 2)
    self.conv = nn.Conv2d(nIn, nOut, k_size, stride = stride, padding = padding, bias = False, dilation = d, groups = groups)
  
  def forward(self, input):
    output = self.conv(input)

    return output

#拡張畳み込み＋バッチ正規化

class CDilated(nn.Module):
  def __init__(self, nIn, nOut, k_size, stride = 1, d = 1, groups = 1):
    super().__init__()
    padding = int((k_size - 1) / 2)
    self.conv = nn.Conv2d(nIn, nOut, k_size, stride = stride, padding = padding, bias = False, dilation = d, groups = groups)
    self.bn = nn.BatchNorm2d(nOut)
  
  def forward(self, input):
    output = self.conv(input)
    output = self.bn(output)

    return output

"""
from torch.nn import init
import torch.nn.functional as F
from cnn.cnn_utils import *
import math
import torch

#EESPモジュール
class EESP(nn.Module):
  def __init__(self, nIn, nOut, stride = 1, k = 4, rf_lim = 7, down_method = 'esp'):
    super().__init__()
    self.stride = stride
    n = int(nOut / k)
    n1 = nOUt - (k - 1) * n
    self.proj_11 = CBR(nIn, n, k_size = 1, stride = 1, groups = k)

    map_receptive_ksize = {3: 1, 5: 2, 7: 3, 9: 4, 11: 5, 13: 6, 15: 7, 17: 8}
    self.k_sizes = list()
    for i in range(k):
      ksize = int(3 + 2 * i)
      ksize = ksize if ksize <= rf_lim else 3
      self.k_sizes.append(ksize)
    self.k_sizes.sort()

    self.spp_dw = nn.ModuleList()
    for i in range(k):
      d_rate = map_receptive_ksize[self.k_sizes[i]]
      self.spp_dw.append(CDilated(n, n, k_size = 3, stride = stride, groups = n, d = d_rate))
    
    self.conv_11_exp = CB(nOut, nOUt, 1, 1, groups = k)
    self.br_after_cat = BR(nOUt)
    self.module_act = nn.PReLU(nOUt)
    #self.downAvg = True if down_method == 'avg' else = False

  def forward(self, input):
    output1 = self.proj_11(input)
    output = [self.spp_dw[0](output1)]

    for k in range(1, len(self.spp_dw)):
      out_k = self.spp_dw[k](output1)
      out_k = out_k + output[k - 1]
      output.append(out_k)
    
    expanded = self.conv_11_exp(self.br_after_cat(torch.cat(output, 1)))
    del output

    if self.stride == 2 and self.downAvg:
      return expanded
    
    if expanded.size() == input.size():
      expanded = expanded + input
    
    return self.module_act(expanded)

#avg.pooling＋EESP(strided2)＋long range connection
class DownSampler(nn.Module):
  def __init__(self, nIn, nOut, k = 4, rf_lim = 9, reinf = True):
    super().__init__()
    nOut_new = nOut - nIn
    self.eesp = EESP(nIn, nOut_new, stride = 2, k = k, rf_lim = rf_lim, down_method = 'avg')
    self.avg = nn.AvgPool2d(k_size = 3, padding = 1, stride = 2)
    
    if reinf:
      self.inp_reinf = nn.Sequential(
          CBR(config_inp_reinf, config_inp_reinf, 3, 1),
          CB(config_inp_reinf, nOUt, 1, 1)
      )
    self.act = PReLU(nOut)

  def forward(self, input, input2 = None):
    avg_out = self.avg(input)
    eesp_out = self.eesp(input)
    output = torch.cat([avg_out, eesp_out], 1)

    if input2 is not None:
      w1 = avg_out.size(2)
      while True:
        input2 = F.avg_pool2d(input2, k_size = 3, padding = 1, stride = 2)
        w2 = input2.size(2)
        if w1 == w2:
          break
    output = output + self.inp_reinf(input2)
    
    return self.act(output)

#EESPNetv2 for ImageNet classification
class EESPNet(nn.Module):
  def __init__(self, classes = 20, s = 1):
    super().__init__()
    reps = [0, 3, 7, 3]
    channels = 3

    rf_lim = [13, 11, 9, 7, 5]
    K = [4] * len(rf_lim)

    base = 32
    config_len = 5
    config = [32, 64, 128, 256, 512, 1024]
    base_s = 0

    global config_inp_reinf
    config_inp_reinf = 3
    self.input_reinforcement = True

    self.level1 = CBR(channels, config[0], 3, 2)

    self.level2 = DownSampler(config[0], config[1], k = K[0], rf_lim = rf_lim[0], reinf = self.input_reinforcement)

    self.level3_0 = DownSampler(config[1], config[2], k = K[0], rf_lim = rf_lim[1], reinf = self.input_reinforcement)
    self.level3 = nn.ModuleList()
    for i in range(reps[1]):
      self.level3.append(ESSP(config[2], config[2], stride = 1, k = K[2], rf_lim = rf_lim[2]))

    self.level4_0 = DownSampler(config[2], config[3], k = K[2], rf_lim = rf_lim[2], reinf = self.input_reinforcement)
    self.level4 = nn.ModuleList()
    for i in range(reps[2]):
      self.level4.append(ESSP(config[3], config[3], stride = 1, k = K[2], rf_lim = rf_lim[3]))

    self.level5_0 = DownSampler(config[3], config[4], k = K[3], rf_lim = rf_lim[3])
    self.level5 = nn.ModuleList()
    for i in range(reps[3]):
      self.level5.append(ESSP(config[4], config[4], stride = 1, k = K[4], rf_lim = rf_lim[4]))
    self.level5.append(CBR(config[4], config[4], 3, 1, groups = config[4]))
    self.level5.append(CBR(config[4], config[5], 1, 1, groups = K[4]))

    self.classifier = nn.Linear(config[5], classes)
    self.init_params()

  def init_params(self):
    for m in self.modules():
      if isinstance(m, nn.Conv2d):
        init.kaiming_normal_(m.weight, mode = 'fan_out')
        if m.bias is not None:
          init.constant_(m.bias, 0)
      elif isinstance(m, nn.BatchNorm2d):
        init.constant_(m.weight, 0)
        init.constant_(m.bias, 0)
      elif isinstance(m, nn.Linear):
        init.normal_(m.weight, std = 0.001)
        if m.bias is not None:
          init.contant_(m.bias, 0)
  def forward(self, input, p = 0.2, seg = True):
    out_l1 = self.level1(input)

    if not self.input_reinforcement:
      del input
      input = None
    
    out_l2 = self.level2(out_l1, input)
    out_l3_0 = self.level3_0(out_l2, input)

    for i, layer in enumerate(self.level3):
      if i == 0:
        out_l3 = layer(out_l3_0)
      else:
        out_l3 = layer(out_l3)
    
    out_l4_0 = self.level4_0(out_l3, input)
    for i, layer in enumerate(self.level4):
      if i == 0:
        out_l4 = layer(out_l4_0)
      else:
        out_l4 = layer(out_l4)
    
    out_l5_0 = self.level5_0(out_l4)
    for i, layer in enumerate(self.level5):
      if i == 0:
        out_l5 = layer(out_l5_0)
      else:
        out_l5 = layer(out_l5)
      
    
    output_g = F.adaptive_avg_pool2d(out_l5, output_size = 1)
    output_g = F.dropout(output_g, p = p, training = self.training)
    output_11 = output_g.view(output_g.size(0), -1)

    return self.classifier(output_11)
        
"""

